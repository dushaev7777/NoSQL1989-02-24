# Задание №3 #

Необходимо:

1) построить шардированный кластер из 3 кластерных нод( по 3 инстанса с репликацией) и с кластером конфига(3 инстанса);

2) добавить балансировку, нагрузить данными, выбрать хороший ключ шардирования, посмотреть как данные перебалансируются между шардами;

3) поронять разные инстансы, посмотреть, что будет происходить, поднять обратно. Описать что произошло.

4) настроить аутентификацию и многоролевой доступ;

# Решение #

1) Для построения шардированного кластера будут использоваться 5 ВМ на ОС CentOS Linux release 7.9.2009 (Core) c установленной СУБД MongoDB созданных в Hyper-V:
   * MongoCFG (192.168.0.210) - нода с экземплярами кластера конфига
   * MongoSRD01 (192.168.0.211) - нода для репликации с 2мя экземплярами 1ого и 2ого шарда
   * MongoSRD02 (192.168.0.212) - нода для репликации с 2мя экземплярами 1ого и 2ого шарда
   * MongoArbiter (192.168.0.214) - нода с Арбитром для голосования 1ого и 2ого шарда
   * MongoRouter (192.168.0.213) - нода с балансировщиком

Добавим адреса на каждой ВМ в файл etc/hosts:
<pre><code>192.168.0.210   MongoCFG.localdomain
192.168.0.211   MongoSRD01.localdomain
192.168.0.212   MongoSRD02.localdomain
192.168.0.213   MongoRouter.localdomain
192.168.0.214   MongoArbiter.localdomain
</code></pre>

Также добавим порты на каждом файрволле для связей между ВМ:
<pre><code>firewall-cmd --permanent --add-port={27000,27001,27002,27003,27011,27012,27013,27100}/tcp
firewall-cmd --reload</code></pre>

1.1) На ВМ MongoCFG создадим каталоги для хранения файлов запущенных экземпляров mongod:
<pre><code>sudo mkdir /home/mongo && sudo mkdir /home/mongo/{dbc1,dbc2,dbc3} && sudo chmod 777 /home/mongo/{dbc1,dbc2,dbc3}</code></pre>
Запустим экземпляры репликасета с ролью конфигурации шарда:
<pre><code>[root@MongoCFG ~]# mongod --configsvr --dbpath /home/mongo/dbc1 --port 27001 --bind_ip_all --replSet RScfg --fork --logpath /home/mongo/dbc1/dbc1.log --pidfilepath /home/mongo/dbc1/dbc1.pid
about to fork child process, waiting until server is ready for connections.
forked process: 1795
child process started successfully, parent exiting
[root@MongoCFG ~]# mongod --configsvr --dbpath /home/mongo/dbc2 --port 27002 --bind_ip_all --replSet RScfg --fork --logpath /home/mongo/dbc2/dbc2.log --pidfilepath /home/mongo/dbc2/dbc2.pid
about to fork child process, waiting until server is ready for connections.
forked process: 1889
child process started successfully, parent exiting
[root@MongoCFG ~]# mongod --configsvr --dbpath /home/mongo/dbc3 --port 27003 --bind_ip_all --replSet RScfg --fork --logpath /home/mongo/dbc3/dbc3.log --pidfilepath /home/mongo/dbc3/dbc3.pid
about to fork child process, waiting until server is ready for connections.
forked process: 2006
child process started successfully, parent exiting</code></pre>
Проверим что экземпляры запущены:
<pre><code>[root@MongoCFG ~]# ps aux | grep mongo| grep -Ev "grep"
mongod    1122  0.9  8.0 2696708 145828 ?      Ssl  09:50   0:30 /usr/bin/mongod -f /etc/mongod.conf
root      1795  2.6  7.9 3436196 145092 ?      Sl   10:41   0:02 mongod --configsvr --dbpath /home/mongo/dbc1 --port 27001 --replSet RScfg --fork --logpath /home/mongo/dbc1/dbc1.log --pidfilepath /home/mongo/dbc1/dbc1.pid
root      1889  2.4  7.7 3311188 140152 ?      Sl   10:41   0:02 mongod --configsvr --dbpath /home/mongo/dbc2 --port 27002 --replSet RScfg --fork --logpath /home/mongo/dbc2/dbc2.log --pidfilepath /home/mongo/dbc2/dbc2.pid
root      2006  2.4  7.7 3304020 139980 ?      Sl   10:41   0:02 mongod --configsvr --dbpath /home/mongo/dbc3 --port 27003 --replSet RScfg --fork --logpath /home/mongo/dbc3/dbc3.log --pidfilepath /home/mongo/dbc3/dbc3.pid</code></pre>
Заходим на экземпляр mongod с портом 27001 и инициализируем репликасет: 
<pre><code>[root@MongoCFG ~]# mongosh --port 27001
test> rs.initiate({"_id" : "RScfg", configsvr: true, members : [{"_id" : 0, priority : 3, host : "MongoCFG.localdomain:27001"},{"_id" : 1, host : "MongoCFG.localdomain:27002"},{"_id" : 2, host : "MongoCFG.localdomain:27003"}]});
</code></pre>
Проверим статус репликасета:
<pre><code>RScfg [direct: primary] test> rs.status()
{
  set: 'RScfg',
  date: ISODate("2023-10-24T14:46:14.715Z"),
  myState: 1,
  term: Long("2"),
  syncSourceHost: '',
  syncSourceId: -1,
  configsvr: true,
  heartbeatIntervalMillis: Long("2000"),
  majorityVoteCount: 2,
  writeMajorityCount: 2,
  votingMembersCount: 3,
  writableVotingMembersCount: 3,
  optimes: {
    lastCommittedOpTime: { ts: Timestamp({ t: 1698158773, i: 1 }), t: Long("2") },
    lastCommittedWallTime: ISODate("2023-10-24T14:46:13.727Z"),
    readConcernMajorityOpTime: { ts: Timestamp({ t: 1698158773, i: 1 }), t: Long("2") },
    appliedOpTime: { ts: Timestamp({ t: 1698158773, i: 1 }), t: Long("2") },
    durableOpTime: { ts: Timestamp({ t: 1698158773, i: 1 }), t: Long("2") },
    lastAppliedWallTime: ISODate("2023-10-24T14:46:13.727Z"),
    lastDurableWallTime: ISODate("2023-10-24T14:46:13.727Z")
  },
  lastStableRecoveryTimestamp: Timestamp({ t: 1698158727, i: 1 }),
  electionCandidateMetrics: {
    lastElectionReason: 'priorityTakeover',
    lastElectionDate: ISODate("2023-10-24T14:39:49.399Z"),
    electionTerm: Long("2"),
    lastCommittedOpTimeAtElection: { ts: Timestamp({ t: 1698158388, i: 1 }), t: Long("1") },
    lastSeenOpTimeAtElection: { ts: Timestamp({ t: 1698158388, i: 1 }), t: Long("1") },
    numVotesNeeded: 2,
    priorityAtElection: 3,
    electionTimeoutMillis: Long("10000"),
    priorPrimaryMemberId: 2,
    numCatchUpOps: Long("0"),
    newTermStartDate: ISODate("2023-10-24T14:39:49.460Z"),
    wMajorityWriteAvailabilityDate: ISODate("2023-10-24T14:39:49.486Z")
  },
  electionParticipantMetrics: {
    votedForCandidate: true,
    electionTerm: Long("1"),
    lastVoteDate: ISODate("2023-10-24T14:39:38.186Z"),
    electionCandidateMemberId: 2,
    voteReason: '',
    lastAppliedOpTimeAtElection: { ts: Timestamp({ t: 1698158367, i: 1 }), t: Long("-1") },
    maxAppliedOpTimeInSet: { ts: Timestamp({ t: 1698158367, i: 1 }), t: Long("-1") },
    priorityAtElection: 3
  },
  members: [
    {
      _id: 0,
      name: 'MongoCFG.localdomain:27001',
      health: 1,
      state: 1,
      stateStr: 'PRIMARY',
      uptime: 520,
      optime: { ts: Timestamp({ t: 1698158773, i: 1 }), t: Long("2") },
      optimeDate: ISODate("2023-10-24T14:46:13.000Z"),
      lastAppliedWallTime: ISODate("2023-10-24T14:46:13.727Z"),
      lastDurableWallTime: ISODate("2023-10-24T14:46:13.727Z"),
      syncSourceHost: '',
      syncSourceId: -1,
      infoMessage: '',
      electionTime: Timestamp({ t: 1698158389, i: 2 }),
      electionDate: ISODate("2023-10-24T14:39:49.000Z"),
      configVersion: 1,
      configTerm: 2,
      self: true,
      lastHeartbeatMessage: ''
    },
    {
      _id: 1,
      name: 'MongoCFG.localdomain:27002',
      health: 1,
      state: 2,
      stateStr: 'SECONDARY',
      uptime: 407,
      optime: { ts: Timestamp({ t: 1698158773, i: 1 }), t: Long("2") },
      optimeDurable: { ts: Timestamp({ t: 1698158772, i: 1 }), t: Long("2") },
      optimeDate: ISODate("2023-10-24T14:46:13.000Z"),
      optimeDurableDate: ISODate("2023-10-24T14:46:12.000Z"),
      lastAppliedWallTime: ISODate("2023-10-24T14:46:13.727Z"),
      lastDurableWallTime: ISODate("2023-10-24T14:46:13.727Z"),
      lastHeartbeat: ISODate("2023-10-24T14:46:13.758Z"),
      lastHeartbeatRecv: ISODate("2023-10-24T14:46:13.747Z"),
      pingMs: Long("0"),
      lastHeartbeatMessage: '',
      syncSourceHost: 'MongoCFG.localdomain:27003',
      syncSourceId: 2,
      infoMessage: '',
      configVersion: 1,
      configTerm: 2
    },
    {
      _id: 2,
      name: 'MongoCFG.localdomain:27003',
      health: 1,
      state: 2,
      stateStr: 'SECONDARY',
      uptime: 407,
      optime: { ts: Timestamp({ t: 1698158773, i: 1 }), t: Long("2") },
      optimeDurable: { ts: Timestamp({ t: 1698158772, i: 1 }), t: Long("2") },
      optimeDate: ISODate("2023-10-24T14:46:13.000Z"),
      optimeDurableDate: ISODate("2023-10-24T14:46:12.000Z"),
      lastAppliedWallTime: ISODate("2023-10-24T14:46:13.727Z"),
      lastDurableWallTime: ISODate("2023-10-24T14:46:13.727Z"),
      lastHeartbeat: ISODate("2023-10-24T14:46:13.759Z"),
      lastHeartbeatRecv: ISODate("2023-10-24T14:46:14.057Z"),
      pingMs: Long("0"),
      lastHeartbeatMessage: '',
      syncSourceHost: 'MongoCFG.localdomain:27001',
      syncSourceId: 0,
      infoMessage: '',
      configVersion: 1,
      configTerm: 2
    }
  ],
  ok: 1,
  '$clusterTime': {
    clusterTime: Timestamp({ t: 1698158773, i: 1 }),
    signature: {
      hash: Binary.createFromBase64("AAAAAAAAAAAAAAAAAAAAAAAAAAA=", 0),
      keyId: Long("0")
    }
  },
  operationTime: Timestamp({ t: 1698158773, i: 1 })
}</code></pre>

1.2) На ВМ MongoSRD01, MongoSRD02, MongoArbiter также необходимо создать каталоги для экземпляров:
<pre><code>sudo mkdir /home/mongo && sudo mkdir /home/mongo/{sh1,sh2} && sudo chmod 777 /home/mongo/{sh1,sh2}
sh1 - экземпляры которые будут относиться к шарду №1
sh2 - экземпляры которые будут относиться к шарду №2</code></pre>
На каждой ВМ начнем запускать экземпляры шардов:
   * MongoSRD01 - <pre><code>[root@MongoSRD01 ~]# mongod --shardsvr --dbpath /home/mongo/sh1 --port 27011 --replSet RS1 --fork --logpath /home/mongo/sh1/sh1.log --pidfilepath /home/mongo/sh1/sh1.pid
about to fork child process, waiting until server is ready for connections.
forked process: 1802
child process started successfully, parent exiting
[root@MongoSRD01 ~]# mongod --shardsvr --dbpath /home/mongo/sh2 --port 27012 --replSet RS2 --fork --logpath /home/mongo/sh2/sh2.log --pidfilepath /home/mongo/sh2/sh2.pid
about to fork child process, waiting until server is ready for connections.
forked process: 1869
child process started successfully, parent exiting
[root@MongoSRD01 ~]# ps aux | grep mongo| grep -Ev "grep"
mongod    1093  1.0  8.1 2702292 147196 ?      Ssl  09:50   0:55 /usr/bin/mongod -f /etc/mongod.conf
root      1802  2.2  5.7 2895156 104908 ?      Sl   11:12   0:06 mongod --shardsvr --dbpath /home/mongo/sh1 --port 27011 --replSet RS1 --fork --logpath /home/mongo/sh1/sh1.log --pidfilepath /home/mongo/sh1/sh1.pid
root      1869  2.2  5.3 2895048 97276 ?       Sl   11:12   0:06 mongod --shardsvr --dbpath /home/mongo/sh2 --port 27012 --replSet RS2 --fork --logpath /home/mongo/sh2/sh2.log --pidfilepath /home/mongo/sh2/sh2.pid
</code></pre>
   * MongoSRD02 - <pre><code>[root@MongoSRD02 ~]# mongod --shardsvr --dbpath /home/mongo/sh1 --port 27011 --replSet RS1 --fork --logpath /home/mongo/sh1/sh1.log --pidfilepath /home/mongo/sh1/sh1.pid
mongod --shardsvr --dbpath /home/mongo/sh2 --port 27012 --replSet RS2 --fork --logpath /home/mongo/sh2/sh2.log --pidfilepath /home/mongo/sh2/sh2.pidabout to fork child process, waiting until server is ready for connections.
forked process: 1840
child process started successfully, parent exiting
[root@MongoSRD02 ~]# mongod --shardsvr --dbpath /home/mongo/sh2 --port 27012 --replSet RS2 --fork --logpath /home/mongo/sh2/sh2.log --pidfilepath /home/mongo/sh2/sh2.pid
about to fork child process, waiting until server is ready for connections.
forked process: 1907
child process started successfully, parent exiting
[root@MongoSRD02 ~]# ps aux | grep mongo| grep -Ev "grep"
mongod    1125  1.0  8.0 2696608 145944 ?      Ssl  09:51   0:58 /usr/bin/mongod -f /etc/mongod.conf
root      1840  2.2  5.7 2895328 105340 ?      Sl   11:12   0:09 mongod --shardsvr --dbpath /home/mongo/sh1 --port 27011 --replSet RS1 --fork --logpath /home/mongo/sh1/sh1.log --pidfilepath /home/mongo/sh1/sh1.pid
root      1907  2.2  5.8 2895328 106852 ?      Sl   11:12   0:09 mongod --shardsvr --dbpath /home/mongo/sh2 --port 27012 --replSet RS2 --fork --logpath /home/mongo/sh2/sh2.log --pidfilepath /home/mongo/sh2/sh2.pid</code></pre>
   * MongoArbiter - <pre><code>[root@MongoArbiter ~]# mongod --shardsvr --dbpath /home/mongo/sh1 --port 27011 --replSet RS1 --fork --logpath /home/mongo/sh1/sh1.log --pidfilepath /home/mongo/sh1/sh1.pid
about to fork child process, waiting until server is ready for connections.
forked process: 1818
child process started successfully, parent exiting
[root@MongoArbiter ~]# mongod --shardsvr --dbpath /home/mongo/sh2 --port 27012 --replSet RS2 --fork --logpath /home/mongo/sh2/sh2.log --pidfilepath /home/mongo/sh2/sh2.pid
about to fork child process, waiting until server is ready for connections.
forked process: 1885
child process started successfully, parent exiting
[root@MongoArbiter ~]# ps aux | grep mongo| grep -Ev "grep"
mongod    1123  1.1  8.1 2696708 148136 ?      Ssl  09:51   1:00 /usr/bin/mongod -f /etc/mongod.conf
root      1818  2.2  5.9 2895048 108572 ?      Sl   11:12   0:12 mongod --shardsvr --dbpath /home/mongo/sh1 --port 27011 --replSet RS1 --fork --logpath /home/mongo/sh1/sh1.log --pidfilepath /home/mongo/sh1/sh1.pid
root      1885  2.2  5.9 2895048 108496 ?      Sl   11:12   0:12 mongod --shardsvr --dbpath /home/mongo/sh2 --port 27012 --replSet RS2 --fork --logpath /home/mongo/sh2/sh2.log --pidfilepath /home/mongo/sh2/sh2.pid
</code></pre>

1.3) Подключаемся к экземпляру 1ого шарда на порт 27011 и пробуем инициализировать кластер:
<pre><code>[root@MongoSRD01 ~]# mongosh --port 27011

test> rs.initiate({"_id" : "RS1", members : [{"_id" : 0, priority : 3, host : "MongoSRD01.localdomain:27011"},{"_id" : 1, host : "MongoSRD02.localdomain:27011"},{"_id" : 2, host : "MongoArbiter.localdomain:27011", arbiterOnly : true}]});
MongoServerError: replSetInitiate quorum check failed because not all proposed set members responded affirmatively: MongoArbiter.localdomain:27011 failed with Error connecting to MongoArbiter.localdomain:27011 (192.168.0.214:27011) :: caused by :: onInvoke :: caused by :: Connection refused, MongoSRD02.localdomain:27011 failed with Error connecting to MongoSRD02.localdomain:27011 (192.168.0.212:27011) :: caused by :: onInvoke :: caused by :: Connection refused</code></pre>

Выдало ошибку, потому что по умолчанию стоит 127.0.0.1 для клиентских соединений, добавим в строку запуска экземплеров --bind_ip 0.0.0.0 (слушать со всех подключений) и перезапустим экземпляры на всех ВМ.
Повторно пробуем инициализировать 1ый шард-кластер:
<pre><code>test> rs.initiate({"_id" : "RS1", members : [{"_id" : 0, priority : 3, host : "MongoSRD01.localdomain:27011"},{"_id" : 1, host : "MongoSRD02.localdomain:27011"},{"_id" : 2, host : "MongoArbiter.localdomain:27011", arbiterOnly : true}]});
{ ok: 1 }
RS1 [direct: primary] test> rs.status()
{
  set: 'RS1',
  date: ISODate("2023-10-24T08:59:25.311Z"),
  myState: 1,
  term: Long("1"),
  syncSourceHost: '',
  syncSourceId: -1,
  heartbeatIntervalMillis: Long("2000"),
  majorityVoteCount: 2,
  writeMajorityCount: 2,
  votingMembersCount: 3,
  writableVotingMembersCount: 2,
  optimes: {
    lastCommittedOpTime: { ts: Timestamp({ t: 1698137957, i: 1 }), t: Long("1") },
    lastCommittedWallTime: ISODate("2023-10-24T08:59:17.273Z"),
    readConcernMajorityOpTime: { ts: Timestamp({ t: 1698137957, i: 1 }), t: Long("1") },
    appliedOpTime: { ts: Timestamp({ t: 1698137957, i: 1 }), t: Long("1") },
    durableOpTime: { ts: Timestamp({ t: 1698137957, i: 1 }), t: Long("1") },
    lastAppliedWallTime: ISODate("2023-10-24T08:59:17.273Z"),
    lastDurableWallTime: ISODate("2023-10-24T08:59:17.273Z")
  },
  lastStableRecoveryTimestamp: Timestamp({ t: 1698137937, i: 1 }),
  electionCandidateMetrics: {
    lastElectionReason: 'electionTimeout',
    lastElectionDate: ISODate("2023-10-24T08:51:17.159Z"),
    electionTerm: Long("1"),
    lastCommittedOpTimeAtElection: { ts: Timestamp({ t: 1698137466, i: 1 }), t: Long("-1") },
    lastSeenOpTimeAtElection: { ts: Timestamp({ t: 1698137466, i: 1 }), t: Long("-1") },
    numVotesNeeded: 2,
    priorityAtElection: 3,
    electionTimeoutMillis: Long("10000"),
    numCatchUpOps: Long("0"),
    newTermStartDate: ISODate("2023-10-24T08:51:17.203Z"),
    wMajorityWriteAvailabilityDate: ISODate("2023-10-24T08:51:17.758Z")
  },
  members: [
    {
      _id: 0,
      name: 'MongoSRD01.localdomain:27011',
      health: 1,
      state: 1,
      stateStr: 'PRIMARY',
      uptime: 545,
      optime: { ts: Timestamp({ t: 1698137957, i: 1 }), t: Long("1") },
      optimeDate: ISODate("2023-10-24T08:59:17.000Z"),
      lastAppliedWallTime: ISODate("2023-10-24T08:59:17.273Z"),
      lastDurableWallTime: ISODate("2023-10-24T08:59:17.273Z"),
      syncSourceHost: '',
      syncSourceId: -1,
      infoMessage: '',
      electionTime: Timestamp({ t: 1698137477, i: 1 }),
      electionDate: ISODate("2023-10-24T08:51:17.000Z"),
      configVersion: 1,
      configTerm: 1,
      self: true,
      lastHeartbeatMessage: ''
    },
    {
      _id: 1,
      name: 'MongoSRD02.localdomain:27011',
      health: 1,
      state: 2,
      stateStr: 'SECONDARY',
      uptime: 499,
      optime: { ts: Timestamp({ t: 1698137957, i: 1 }), t: Long("1") },
      optimeDurable: { ts: Timestamp({ t: 1698137957, i: 1 }), t: Long("1") },
      optimeDate: ISODate("2023-10-24T08:59:17.000Z"),
      optimeDurableDate: ISODate("2023-10-24T08:59:17.000Z"),
      lastAppliedWallTime: ISODate("2023-10-24T08:59:17.273Z"),
      lastDurableWallTime: ISODate("2023-10-24T08:59:17.273Z"),
      lastHeartbeat: ISODate("2023-10-24T08:59:23.651Z"),
      lastHeartbeatRecv: ISODate("2023-10-24T08:59:24.733Z"),
      pingMs: Long("0"),
      lastHeartbeatMessage: '',
      syncSourceHost: 'MongoSRD01.localdomain:27011',
      syncSourceId: 0,
      infoMessage: '',
      configVersion: 1,
      configTerm: 1
    },
    {
      _id: 2,
      name: 'MongoArbiter.localdomain:27011',
      health: 1,
      state: 7,
      stateStr: 'ARBITER',
      uptime: 499,
      lastHeartbeat: ISODate("2023-10-24T08:59:23.654Z"),
      lastHeartbeatRecv: ISODate("2023-10-24T08:59:23.654Z"),
      pingMs: Long("0"),
      lastHeartbeatMessage: '',
      syncSourceHost: '',
      syncSourceId: -1,
      infoMessage: '',
      configVersion: 1,
      configTerm: 1
    }
  ],
  ok: 1,
  '$clusterTime': {
    clusterTime: Timestamp({ t: 1698137957, i: 1 }),
    signature: {
      hash: Binary.createFromBase64("AAAAAAAAAAAAAAAAAAAAAAAAAAA=", 0),
      keyId: Long("0")
    }
  },
  operationTime: Timestamp({ t: 1698137957, i: 1 })
}
</code></pre>
Подключаемся к экземпляру где будет инициализирован 2ой шард:
<pre><code>[root@MongoSRD01 ~]# mongosh --port 27012

test> rs.initiate({"_id" : "RS2", members : [{"_id" : 0, priority : 3, host : "MongoSRD01.localdomain:27012"},{"_id" : 1, host : "MongoSRD02.localdomain:27012"},{"_id" : 2, host : "MongoArbiter.localdomain:27012", arbiterOnly : true}]});
{ ok: 1 }
RS2 [direct: primary] test> rs.status()
{
  set: 'RS2',
  date: ISODate("2023-10-24T09:05:47.971Z"),
  myState: 1,
  term: Long("1"),
  syncSourceHost: '',
  syncSourceId: -1,
  heartbeatIntervalMillis: Long("2000"),
  majorityVoteCount: 2,
  writeMajorityCount: 2,
  votingMembersCount: 3,
  writableVotingMembersCount: 2,
  optimes: {
    lastCommittedOpTime: { ts: Timestamp({ t: 1698138345, i: 1 }), t: Long("1") },
    lastCommittedWallTime: ISODate("2023-10-24T09:05:45.595Z"),
    readConcernMajorityOpTime: { ts: Timestamp({ t: 1698138345, i: 1 }), t: Long("1") },
    appliedOpTime: { ts: Timestamp({ t: 1698138345, i: 1 }), t: Long("1") },
    durableOpTime: { ts: Timestamp({ t: 1698138345, i: 1 }), t: Long("1") },
    lastAppliedWallTime: ISODate("2023-10-24T09:05:45.595Z"),
    lastDurableWallTime: ISODate("2023-10-24T09:05:45.595Z")
  },
  lastStableRecoveryTimestamp: Timestamp({ t: 1698138304, i: 1 }),
  electionCandidateMetrics: {
    lastElectionReason: 'electionTimeout',
    lastElectionDate: ISODate("2023-10-24T09:05:15.467Z"),
    electionTerm: Long("1"),
    lastCommittedOpTimeAtElection: { ts: Timestamp({ t: 1698138304, i: 1 }), t: Long("-1") },
    lastSeenOpTimeAtElection: { ts: Timestamp({ t: 1698138304, i: 1 }), t: Long("-1") },
    numVotesNeeded: 2,
    priorityAtElection: 3,
    electionTimeoutMillis: Long("10000"),
    numCatchUpOps: Long("0"),
    newTermStartDate: ISODate("2023-10-24T09:05:15.542Z"),
    wMajorityWriteAvailabilityDate: ISODate("2023-10-24T09:05:16.056Z")
  },
  members: [
    {
      _id: 0,
      name: 'MongoSRD01.localdomain:27012',
      health: 1,
      state: 1,
      stateStr: 'PRIMARY',
      uptime: 925,
      optime: { ts: Timestamp({ t: 1698138345, i: 1 }), t: Long("1") },
      optimeDate: ISODate("2023-10-24T09:05:45.000Z"),
      lastAppliedWallTime: ISODate("2023-10-24T09:05:45.595Z"),
      lastDurableWallTime: ISODate("2023-10-24T09:05:45.595Z"),
      syncSourceHost: '',
      syncSourceId: -1,
      infoMessage: 'Could not find member to sync from',
      electionTime: Timestamp({ t: 1698138315, i: 1 }),
      electionDate: ISODate("2023-10-24T09:05:15.000Z"),
      configVersion: 1,
      configTerm: 1,
      self: true,
      lastHeartbeatMessage: ''
    },
    {
      _id: 1,
      name: 'MongoSRD02.localdomain:27012',
      health: 1,
      state: 2,
      stateStr: 'SECONDARY',
      uptime: 43,
      optime: { ts: Timestamp({ t: 1698138345, i: 1 }), t: Long("1") },
      optimeDurable: { ts: Timestamp({ t: 1698138345, i: 1 }), t: Long("1") },
      optimeDate: ISODate("2023-10-24T09:05:45.000Z"),
      optimeDurableDate: ISODate("2023-10-24T09:05:45.000Z"),
      lastAppliedWallTime: ISODate("2023-10-24T09:05:45.595Z"),
      lastDurableWallTime: ISODate("2023-10-24T09:05:45.595Z"),
      lastHeartbeat: ISODate("2023-10-24T09:05:47.538Z"),
      lastHeartbeatRecv: ISODate("2023-10-24T09:05:46.552Z"),
      pingMs: Long("0"),
      lastHeartbeatMessage: '',
      syncSourceHost: 'MongoSRD01.localdomain:27012',
      syncSourceId: 0,
      infoMessage: '',
      configVersion: 1,
      configTerm: 1
    },
    {
      _id: 2,
      name: 'MongoArbiter.localdomain:27012',
      health: 1,
      state: 7,
      stateStr: 'ARBITER',
      uptime: 43,
      lastHeartbeat: ISODate("2023-10-24T09:05:47.538Z"),
      lastHeartbeatRecv: ISODate("2023-10-24T09:05:47.537Z"),
      pingMs: Long("0"),
      lastHeartbeatMessage: '',
      syncSourceHost: '',
      syncSourceId: -1,
      infoMessage: '',
      configVersion: 1,
      configTerm: 1
    }
  ],
  ok: 1,
  '$clusterTime': {
    clusterTime: Timestamp({ t: 1698138345, i: 1 }),
    signature: {
      hash: Binary.createFromBase64("AAAAAAAAAAAAAAAAAAAAAAAAAAA=", 0),
      keyId: Long("0")
    }
  },
  operationTime: Timestamp({ t: 1698138345, i: 1 })
}
</code></pre>

2) Настраиваем маршрутизатор запросов MongoRouter:
<pre><code>sudo mkdir /home/mongo && sudo mkdir /home/mongo/{dbc1,dbc2} && sudo chmod 777 /home/mongo/{dbc1,dbc2}
mongos --configdb RScfg/MongoCFG.localdomain:27001,MongoCFG.localdomain:27002,MongoCFG.localdomain:27003 --port 27000 --bind_ip_all --fork --logpath /home/mongo/dbc1/dbs1.log --pidfilepath /home/mongo/dbc1/dbs1.pid 
mongos --configdb RScfg/MongoCFG.localdomain:27001,MongoCFG.localdomain:27002,MongoCFG.localdomain:27003 --port 27100 --bind_ip_all --fork --logpath /home/mongo/dbc2/dbs2.log --pidfilepath /home/mongo/dbc2/dbs2.pid</code></pre>
Проверим запущенные экземпляры mongos:
<pre><code>[root@MongoRouter ~]# ps aux | grep mongo| grep -Ev "grep"
root      1898  0.5  1.8 2600968 34328 ?       Sl   08:56   0:06 mongos --configdb RScfg/MongoCFG.localdomain:27001,MongoCFG.localdomain:27002,MongoCFG.localdomain:27003 --port 27000 --bind_ip_all --fork --logpath /home/mongo/dbc1/dbs1.log --pidfilepath /home/mongo/dbc1/dbs1.pid
root      2398  0.4  1.7 2598948 32396 ?       Sl   09:05   0:03 mongos --configdb RScfg/MongoCFG.localdomain:27001,MongoCFG.localdomain:27002,MongoCFG.localdomain:27003 --port 27100 --bind_ip_all --fork --logpath /home/mongo/dbc2/dbs2.log --pidfilepath /home/mongo/dbc2/dbs2.pid</code></pre>
Добавим шарды в mongos:
<pre><code>[root@MongoRouter ~]# mongosh --port 27000
Current Mongosh Log ID: 6538b3dec8dcf3627ee3d1b6
Connecting to:          mongodb://127.0.0.1:27000/?directConnection=true&serverSelectionTimeoutMS=2000&appName=mongosh+2.0.1
Using MongoDB:          7.0.2
Using Mongosh:          2.0.1
[direct: mongos] test> sh.addShard("RS1/MongoSRD01.localdomain:27011,MongoSRD02.localdomain:27011,MongoArbiter.localdomain:27011")
MongoServerError: Cannot add RS1/MongoSRD01.localdomain:27011,MongoSRD02.localdomain:27011,MongoArbiter.localdomain:27011 as a shard since the implicit default write concern on this shard is set to {w : 1}, because number of arbiters in the shard's configuration caused the number of writable voting members not to be strictly more than the voting majority. Change the shard configuration or set the cluster-wide write concern using the setDefaultRWConcern command and try again.</code></pre>
Ошибка появилась поскольку неявная проблема записи по умолчанию для этого сегмента установлена в {w : 1}, поскольку количество арбитров в конфигурации сегмента привело к увеличению количества число участников с правом голоса, доступных для записи, строго не должно превышать большинства голосующих. Необходимо установить ограничение на запись в масштабах кластера.
<pre><code>[direct: mongos] test> db.adminCommand({ "setDefaultRWConcern": 1, "defaultWriteConcern": { "w": 1 }, "defaultReadConcern": { "level": "majority" } })
{
  defaultReadConcern: { level: 'majority' },
  defaultWriteConcern: { w: 1, wtimeout: 0 },
  updateOpTime: Timestamp({ t: 1698215767, i: 1 }),
  updateWallClockTime: ISODate("2023-10-25T06:36:08.877Z"),
  defaultWriteConcernSource: 'global',
  defaultReadConcernSource: 'global',
  localUpdateWallClockTime: ISODate("2023-10-25T06:36:08.877Z"),
  ok: 1,
  '$clusterTime': {
    clusterTime: Timestamp({ t: 1698215768, i: 1 }),
    signature: {
      hash: Binary.createFromBase64("AAAAAAAAAAAAAAAAAAAAAAAAAAA=", 0),
      keyId: Long("0")
    }
  },
  operationTime: Timestamp({ t: 1698215768, i: 1 })
}</code></pre>
Повторно добавим шарды к балансировщик:
<pre><code>[direct: mongos] test> sh.addShard("RS1/MongoSRD01.localdomain:27011,MongoSRD02.localdomain:27011,MongoArbiter.localdomain:27011")
{
  shardAdded: 'RS1',
  ok: 1,
  '$clusterTime': {
    clusterTime: Timestamp({ t: 1698215778, i: 5 }),
    signature: {
      hash: Binary.createFromBase64("AAAAAAAAAAAAAAAAAAAAAAAAAAA=", 0),
      keyId: Long("0")
    }
  },
  operationTime: Timestamp({ t: 1698215778, i: 5 })
}
[direct: mongos] test> sh.addShard("RS2/MongoSRD01.localdomain:27012,MongoSRD02.localdomain:27012,MongoArbiter.localdomain:27012")
{
  shardAdded: 'RS2',
  ok: 1,
  '$clusterTime': {
    clusterTime: Timestamp({ t: 1698215808, i: 14 }),
    signature: {
      hash: Binary.createFromBase64("AAAAAAAAAAAAAAAAAAAAAAAAAAA=", 0),
      keyId: Long("0")
    }
  },
  operationTime: Timestamp({ t: 1698215808, i: 4 })
}
</code></pre>
Проверим статус mongos:
<pre><code>[direct: mongos] test> sh.status()
shardingVersion
{ _id: 1, clusterId: ObjectId("6537d72a111b40cb249f2f6c") }
---
shards
[
  {
    _id: 'RS1',
    host: 'RS1/MongoSRD01.localdomain:27011,MongoSRD02.localdomain:27011',
    state: 1,
    topologyTime: Timestamp({ t: 1698215778, i: 2 })
  },
  {
    _id: 'RS2',
    host: 'RS2/MongoSRD01.localdomain:27012,MongoSRD02.localdomain:27012',
    state: 1,
    topologyTime: Timestamp({ t: 1698215808, i: 2 })
  }
]
---
active mongoses
[ { '7.0.2': 2 } ]
---
autosplit
{ 'Currently enabled': 'yes' }
---
balancer
{
  'Currently running': 'no',
  'Currently enabled': 'yes',
  'Failed balancer rounds in last 5 attempts': 0,
  'Migration Results for the last 24 hours': 'No recent migrations'
}
---
databases
[
  {
    database: { _id: 'config', primary: 'config', partitioned: true },
    collections: {
      'config.system.sessions': {
        shardKey: { _id: 1 },
        unique: false,
        balancing: true,
        chunkMetadata: [ { shard: 'RS1', nChunks: 1 } ],
        chunks: [
          { min: { _id: MinKey() }, max: { _id: MaxKey() }, 'on shard': 'RS1', 'last modified': Timestamp({ t: 1, i: 0 }) }
        ],
        tags: []
      }
    }
  }
]
</code></pre>
Кластера RS1 и RS2 для шардирования добавлены

2.1) Добавим данные на кластер:
<pre><code>[direct: mongos] test> use bank                                # перейдем и создадим БД "bank"
switched to db bank 
[direct: mongos] bank> sh.enableSharding("bank")               # включим шардирование для БД "bank"
{
  ok: 1,
  '$clusterTime': {
    clusterTime: Timestamp({ t: 1698235283, i: 7 }),
    signature: {
      hash: Binary.createFromBase64("AAAAAAAAAAAAAAAAAAAAAAAAAAA=", 0),
      keyId: Long("0")
    }
  },
  operationTime: Timestamp({ t: 1698235283, i: 2 })
}
[direct: mongos] bank> use config 
switched to db config
[direct: mongos] config> db.settings.updateOne(                 # в конфиге выставляем размер чанка в размере 1мб
...    { _id: "chunksize" },
...    { $set: { _id: "chunksize", value: 1 } },
...    { upsert: true }
... )
{
  acknowledged: true,
  insertedId: 'chunksize',
  matchedCount: 0,
  modifiedCount: 0,
  upsertedCount: 1
}
[direct: mongos] config> use bank #возвращаемся в БД "bank"
switched to db bank
[direct: mongos] bank> for (var i=0; i<50000; i++) { db.tickets.insert({name: "Max ammout of cost tickets", amount: Math.random()*100}) }
DeprecationWarning: Collection.insert() is deprecated. Use insertOne, insertMany, or bulkWrite.
{
  acknowledged: true,
  insertedIds: { '0': ObjectId("6539046349c9dcecba694194") }
}</code></pre>

2.2) В данном случае будем шардировать по полю amount:
<pre><code>[direct: mongos] bank> db.tickets.createIndex({amount: 1})  # создадим индекс
amount_1
[direct: mongos] bank> use admin
switched to db admin
[direct: mongos] admin> db.runCommand({shardCollection: "bank.tickets", key: {amount: 1}}) # Зададим ключ шардирования
{
  collectionsharded: 'bank.tickets',
  ok: 1,
  '$clusterTime': {
    clusterTime: Timestamp({ t: 1698240646, i: 5 }),
    signature: {
      hash: Binary.createFromBase64("AAAAAAAAAAAAAAAAAAAAAAAAAAA=", 0),
      keyId: Long("0")
    }
  },
  operationTime: Timestamp({ t: 1698240646, i: 5 })
}</code></pre>
2.3)Проверим распределение данных между шардами:
<pre><code>[direct: mongos] admin> sh.status()
shardingVersion
{ _id: 1, clusterId: ObjectId("6537d72a111b40cb249f2f6c") }
---
shards
[
  {
    _id: 'RS1',
    host: 'RS1/MongoSRD01.localdomain:27011,MongoSRD02.localdomain:27011',
    state: 1,
    topologyTime: Timestamp({ t: 1698215778, i: 2 })
  },
  {
    _id: 'RS2',
    host: 'RS2/MongoSRD01.localdomain:27012,MongoSRD02.localdomain:27012',
    state: 1,
    topologyTime: Timestamp({ t: 1698215808, i: 2 })
  }
]
---
active mongoses
[ { '7.0.2': 2 } ]
---
autosplit
{ 'Currently enabled': 'yes' }
---
balancer
{
  'Currently enabled': 'yes',
  'Failed balancer rounds in last 5 attempts': 0,
  'Currently running': 'no',
  'Migration Results for the last 24 hours': { '1': 'Success' }
}
---
databases
[
  {
    database: {
      _id: 'bank',
      primary: 'RS2',
      partitioned: false,
      version: {
        uuid: new UUID("e5a05322-468e-4928-b161-e091f2a09dac"),
        timestamp: Timestamp({ t: 1698235282, i: 1 }),
        lastMod: 1
      }
    },
    collections: {
      'bank.tickets': {
        shardKey: { amount: 1 },
        unique: false,
        balancing: true,
        chunkMetadata: [ { shard: 'RS1', nChunks: 1 }, { shard: 'RS2', nChunks: 1 } ],
        chunks: [
          { min: { amount: MinKey() }, max: { amount: 28.089354268888655 }, 'on shard': 'RS1', 'last modified': Timestamp({ t: 2, i: 0 }) },
          { min: { amount: 28.089354268888655 }, max: { amount: MaxKey() }, 'on shard': 'RS2', 'last modified': Timestamp({ t: 2, i: 1 }) }
        ],
        tags: []
      }
    }
  },
  {
    database: { _id: 'config', primary: 'config', partitioned: true },
    collections: {
      'config.system.sessions': {
        shardKey: { _id: 1 },
        unique: false,
        balancing: true,
        chunkMetadata: [ { shard: 'RS1', nChunks: 1 } ],
        chunks: [
          { min: { _id: MinKey() }, max: { _id: MaxKey() }, 'on shard': 'RS1', 'last modified': Timestamp({ t: 1, i: 0 }) }
        ],
        tags: []
      }
    }
  }
]</code></pre>
Видно что на шарде RS1 находятся данные от минимального до значения amount = 28.089354268888655, а на шарде RS2 от amount = 28.089354268888655 до максимального. Следовательно данные равномерно распределились по шардам.

3) Проверка отказоустойчивости кластера
Выключим полностью ВМ MongoSRD01:
<pre><code>[root@MongoSRD01 ~]# shutdown now</code></pre>
Проверим статусы репликасетов через MongoSRD02:
<pre><code>[root@MongoSRD02 ~]# mongosh --port 27011
Current Mongosh Log ID: 653a1b82fed93991917e6d52 
RS1 [direct: primary] test> rs.status()
{
  set: 'RS1', -------------------------------------------------1ый репликасет
  date: ISODate("2023-10-26T07:56:20.633Z"),
  myState: 1,
  term: Long("5"),
  syncSourceHost: '',
  syncSourceId: -1,
  heartbeatIntervalMillis: Long("2000"),
  majorityVoteCount: 2,
  writeMajorityCount: 2,
  votingMembersCount: 3,
  writableVotingMembersCount: 2,
  optimes: {
    lastCommittedOpTime: { ts: Timestamp({ t: 1698306707, i: 1 }), t: Long("3") },
    lastCommittedWallTime: ISODate("2023-10-26T07:51:47.089Z"),
    readConcernMajorityOpTime: { ts: Timestamp({ t: 1698306707, i: 1 }), t: Long("3") },
    appliedOpTime: { ts: Timestamp({ t: 1698306976, i: 1 }), t: Long("5") },
    durableOpTime: { ts: Timestamp({ t: 1698306976, i: 1 }), t: Long("5") },
    lastAppliedWallTime: ISODate("2023-10-26T07:56:16.159Z"),
    lastDurableWallTime: ISODate("2023-10-26T07:56:16.159Z")
  },
  lastStableRecoveryTimestamp: Timestamp({ t: 1698306707, i: 1 }),
  electionCandidateMetrics: {
    lastElectionReason: 'electionTimeout',
    lastElectionDate: ISODate("2023-10-26T07:52:04.872Z"),
    electionTerm: Long("5"),
    lastCommittedOpTimeAtElection: { ts: Timestamp({ t: 1698306707, i: 1 }), t: Long("3") },
    lastSeenOpTimeAtElection: { ts: Timestamp({ t: 1698306707, i: 1 }), t: Long("3") },
    numVotesNeeded: 2,
    priorityAtElection: 1,
    electionTimeoutMillis: Long("10000"),
    numCatchUpOps: Long("0"),
    newTermStartDate: ISODate("2023-10-26T07:52:06.129Z")
  },
  electionParticipantMetrics: {
    votedForCandidate: true,
    electionTerm: Long("3"),
    lastVoteDate: ISODate("2023-10-25T06:06:45.171Z"),
    electionCandidateMemberId: 0,
    voteReason: '',
    lastAppliedOpTimeAtElection: { ts: Timestamp({ t: 1698214003, i: 1 }), t: Long("2") },
    maxAppliedOpTimeInSet: { ts: Timestamp({ t: 1698214003, i: 1 }), t: Long("2") },
    priorityAtElection: 1
  },
  members: [
    {
      _id: 0,
      name: 'MongoSRD01.localdomain:27011',
      health: 0,--------------------------------------------------------Сообщает о том что нода не жива
      state: 8,
      stateStr: '(not reachable/healthy)',
      uptime: 0,
      optime: { ts: Timestamp({ t: 0, i: 0 }), t: Long("-1") },
      optimeDurable: { ts: Timestamp({ t: 0, i: 0 }), t: Long("-1") },
      optimeDate: ISODate("1970-01-01T00:00:00.000Z"),
      optimeDurableDate: ISODate("1970-01-01T00:00:00.000Z"),
      lastAppliedWallTime: ISODate("2023-10-26T07:51:47.089Z"),
      lastDurableWallTime: ISODate("2023-10-26T07:51:47.089Z"),
      lastHeartbeat: ISODate("2023-10-26T07:56:16.444Z"),
      lastHeartbeatRecv: ISODate("2023-10-26T07:51:54.633Z"),
      pingMs: Long("0"),
      lastHeartbeatMessage: 'Error connecting to MongoSRD01.localdomain:27011 (192.168.0.211:27011) :: caused by :: onInvoke :: caused by :: No route to host',------------------------------------------Описание означает что нет коннекта
      syncSourceHost: '',
      syncSourceId: -1,
      infoMessage: '',
      configVersion: 1,
      configTerm: 3
    },
    {
      _id: 1,
      name: 'MongoSRD02.localdomain:27011',
      health: 1,
      state: 1,
      stateStr: 'PRIMARY',----------------------------------------------Вторая нода стала первичной
      uptime: 169552,
      optime: { ts: Timestamp({ t: 1698306976, i: 1 }), t: Long("5") },
      optimeDate: ISODate("2023-10-26T07:56:16.000Z"),
      lastAppliedWallTime: ISODate("2023-10-26T07:56:16.159Z"),
      lastDurableWallTime: ISODate("2023-10-26T07:56:16.159Z"),
      syncSourceHost: '',
      syncSourceId: -1,
      infoMessage: '',
      electionTime: Timestamp({ t: 1698306724, i: 1 }),
      electionDate: ISODate("2023-10-26T07:52:04.000Z"),
      configVersion: 1,
      configTerm: 5,
      self: true,
      lastHeartbeatMessage: ''
    },
    {
      _id: 2,
      name: 'MongoArbiter.localdomain:27011',
      health: 1,
      state: 7,
      stateStr: 'ARBITER',
      uptime: 159423,
      lastHeartbeat: ISODate("2023-10-26T07:56:20.406Z"),
      lastHeartbeatRecv: ISODate("2023-10-26T07:56:20.403Z"),
      pingMs: Long("1"),
      lastHeartbeatMessage: '',
      syncSourceHost: '',
      syncSourceId: -1,
      infoMessage: '',
      configVersion: 1,
      configTerm: 5
    }
  ],
  ok: 1,
  '$clusterTime': {
    clusterTime: Timestamp({ t: 1698306976, i: 1 }),
    signature: {
      hash: Binary.createFromBase64("AAAAAAAAAAAAAAAAAAAAAAAAAAA=", 0),
      keyId: Long("0")
    }
  },
  operationTime: Timestamp({ t: 1698306976, i: 1 })
}
[root@MongoSRD02 ~]# mongosh --port 27012
Current Mongosh Log ID: 653a1df78ba53de54ee6a23e
RS2 [direct: primary] test> rs.status()
{
  set: 'RS2',
  date: ISODate("2023-10-26T08:06:29.235Z"),
  myState: 1,
  term: Long("4"),
  syncSourceHost: '',
  syncSourceId: -1,
  heartbeatIntervalMillis: Long("2000"),
  majorityVoteCount: 2,
  writeMajorityCount: 2,
  votingMembersCount: 3,
  writableVotingMembersCount: 2,
  optimes: {
    lastCommittedOpTime: { ts: Timestamp({ t: 1698306707, i: 1 }), t: Long("2") },
    lastCommittedWallTime: ISODate("2023-10-26T07:51:47.089Z"),
    readConcernMajorityOpTime: { ts: Timestamp({ t: 1698306707, i: 1 }), t: Long("2") },
    appliedOpTime: { ts: Timestamp({ t: 1698307586, i: 1 }), t: Long("4") },
    durableOpTime: { ts: Timestamp({ t: 1698307586, i: 1 }), t: Long("4") },
    lastAppliedWallTime: ISODate("2023-10-26T08:06:26.198Z"),
    lastDurableWallTime: ISODate("2023-10-26T08:06:26.198Z")
  },
  lastStableRecoveryTimestamp: Timestamp({ t: 1698306707, i: 1 }),
  electionCandidateMetrics: {
    lastElectionReason: 'electionTimeout',
    lastElectionDate: ISODate("2023-10-26T07:52:04.973Z"),
    electionTerm: Long("4"),
    lastCommittedOpTimeAtElection: { ts: Timestamp({ t: 1698306707, i: 1 }), t: Long("2") },
    lastSeenOpTimeAtElection: { ts: Timestamp({ t: 1698306707, i: 1 }), t: Long("2") },
    numVotesNeeded: 2,
    priorityAtElection: 1,
    electionTimeoutMillis: Long("10000"),
    numCatchUpOps: Long("0"),
    newTermStartDate: ISODate("2023-10-26T07:52:06.103Z")
  },
  electionParticipantMetrics: {
    votedForCandidate: true,
    electionTerm: Long("2"),
    lastVoteDate: ISODate("2023-10-25T06:06:43.826Z"),
    electionCandidateMemberId: 0,
    voteReason: '',
    lastAppliedOpTimeAtElection: { ts: Timestamp({ t: 1698213993, i: 1 }), t: Long("1") },
    maxAppliedOpTimeInSet: { ts: Timestamp({ t: 1698213993, i: 1 }), t: Long("1") },
    priorityAtElection: 1
  },
  members: [
    {
      _id: 0,
      name: 'MongoSRD01.localdomain:27012',
      health: 0,
      state: 8,
      stateStr: '(not reachable/healthy)',
      uptime: 0,
      optime: { ts: Timestamp({ t: 0, i: 0 }), t: Long("-1") },
      optimeDurable: { ts: Timestamp({ t: 0, i: 0 }), t: Long("-1") },
      optimeDate: ISODate("1970-01-01T00:00:00.000Z"),
      optimeDurableDate: ISODate("1970-01-01T00:00:00.000Z"),
      lastAppliedWallTime: ISODate("2023-10-26T07:51:47.089Z"),
      lastDurableWallTime: ISODate("2023-10-26T07:51:47.089Z"),
      lastHeartbeat: ISODate("2023-10-26T08:06:26.667Z"),
      lastHeartbeatRecv: ISODate("2023-10-26T07:51:54.687Z"),
      pingMs: Long("2"),
      lastHeartbeatMessage: 'Error connecting to MongoSRD01.localdomain:27012 (192.168.0.211:27012) :: caused by :: onInvoke :: caused by :: No route to host',
      syncSourceHost: '',
      syncSourceId: -1,
      infoMessage: '',
      configVersion: 1,
      configTerm: 2
    },
    {
      _id: 1,
      name: 'MongoSRD02.localdomain:27012',
      health: 1,
      state: 1,
      stateStr: 'PRIMARY',
      uptime: 170160,
      optime: { ts: Timestamp({ t: 1698307586, i: 1 }), t: Long("4") },
      optimeDate: ISODate("2023-10-26T08:06:26.000Z"),
      lastAppliedWallTime: ISODate("2023-10-26T08:06:26.198Z"),
      lastDurableWallTime: ISODate("2023-10-26T08:06:26.198Z"),
      syncSourceHost: '',
      syncSourceId: -1,
      infoMessage: '',
      electionTime: Timestamp({ t: 1698306724, i: 1 }),
      electionDate: ISODate("2023-10-26T07:52:04.000Z"),
      configVersion: 1,
      configTerm: 4,
      self: true,
      lastHeartbeatMessage: ''
    },
    {
      _id: 2,
      name: 'MongoArbiter.localdomain:27012',
      health: 1,
      state: 7,
      stateStr: 'ARBITER',
      uptime: 1920,
      lastHeartbeat: ISODate("2023-10-26T08:06:29.086Z"),
      lastHeartbeatRecv: ISODate("2023-10-26T08:06:29.077Z"),
      pingMs: Long("3"),
      lastHeartbeatMessage: '',
      syncSourceHost: '',
      syncSourceId: -1,
      infoMessage: '',
      configVersion: 1,
      configTerm: 4
    }
  ],
  ok: 1,
  '$clusterTime': {
    clusterTime: Timestamp({ t: 1698307586, i: 1 }),
    signature: {
      hash: Binary.createFromBase64("AAAAAAAAAAAAAAAAAAAAAAAAAAA=", 0),
      keyId: Long("0")
    }
  },
  operationTime: Timestamp({ t: 1698307586, i: 1 })
}
</code></pre>
На 2ом репликасете аналогичная ситуация, это означает что голосование проведено успешно, Secondary перешел в статус Primary.
Попробуем еще залить данных в коллекцию:
<pre><code>[direct: mongos] bank> for (var i=0; i<100000; i++) { db.tickets.insert({name: "Max ammout of cost tickets", amount: Math.random()*100}) }</code></pre>
Проверим количество общее количество полей в коллекции tickets:
<pre><code>[direct: mongos] bank> db.tickets.count()
DeprecationWarning: Collection.count() is deprecated. Use countDocuments or estimatedDocumentCount.
150000-----------------------------------------------------------------------Поля успешно добавлены</code></pre>
Запустим ВМ MongoSRD01 и экземпляры репликасетов RS1, RS2:
<pre><code>[root@MongoSRD01 ~]# mongod --shardsvr --dbpath /home/mongo/sh1 --port 27011 --bind_ip 0.0.0.0 --replSet RS1 --fork --logpath /home/mongo/sh1/sh1.log --pidfilepath /home/mongo/sh1/sh1.pid
about to fork child process, waiting until server is ready for connections.
forked process: 1721
child process started successfully, parent exiting
[root@MongoSRD01 ~]# mongod --shardsvr --dbpath /home/mongo/sh2 --port 27012 --bind_ip 0.0.0.0 --replSet RS2 --fork --logpath /home/mongo/sh2/sh2.log --pidfilepath /home/mongo/sh2/sh2.pid
about to fork child process, waiting until server is ready for connections.
forked process: 1851
child process started successfully, parent exiting</code></pre>
Проверим статус экземпляров репликасетов на ноде MongoSRD01:
<pre><code>[root@MongoSRD01 ~]# mongosh --port 27011
Current Mongosh Log ID: 653a33ad225634619ea6b84c
RS1 [direct: primary] test> rs.status()
{
  set: 'RS1',
  date: ISODate("2023-10-26T09:38:57.853Z"),
  myState: 1,
  term: Long("6"),
  syncSourceHost: '',
  syncSourceId: -1,
  heartbeatIntervalMillis: Long("2000"),
  majorityVoteCount: 2,
  writeMajorityCount: 2,
  votingMembersCount: 3,
  writableVotingMembersCount: 2,
  optimes: {
    lastCommittedOpTime: { ts: Timestamp({ t: 1698313134, i: 1 }), t: Long("6") },
    lastCommittedWallTime: ISODate("2023-10-26T09:38:54.458Z"),
    readConcernMajorityOpTime: { ts: Timestamp({ t: 1698313134, i: 1 }), t: Long("6") },
    appliedOpTime: { ts: Timestamp({ t: 1698313134, i: 1 }), t: Long("6") },
    durableOpTime: { ts: Timestamp({ t: 1698313134, i: 1 }), t: Long("6") },
    lastAppliedWallTime: ISODate("2023-10-26T09:38:54.458Z"),
    lastDurableWallTime: ISODate("2023-10-26T09:38:54.458Z")
  },
  lastStableRecoveryTimestamp: Timestamp({ t: 1698313124, i: 1 }),
  electionCandidateMetrics: {
    lastElectionReason: 'priorityTakeover',
    lastElectionDate: ISODate("2023-10-26T09:19:04.299Z"),
    electionTerm: Long("6"),
    lastCommittedOpTimeAtElection: { ts: Timestamp({ t: 1698311936, i: 1 }), t: Long("5") },
    lastSeenOpTimeAtElection: { ts: Timestamp({ t: 1698311936, i: 1 }), t: Long("5") },
    numVotesNeeded: 2,
    priorityAtElection: 3,
    electionTimeoutMillis: Long("10000"),
    priorPrimaryMemberId: 1,
    numCatchUpOps: Long("0"),
    newTermStartDate: ISODate("2023-10-26T09:19:04.328Z"),
    wMajorityWriteAvailabilityDate: ISODate("2023-10-26T09:19:04.374Z")
  },
  members: [
    {
      _id: 0,
      name: 'MongoSRD01.localdomain:27011',
      health: 1,
      state: 1,
      stateStr: 'PRIMARY',
      uptime: 1205,
      optime: { ts: Timestamp({ t: 1698313134, i: 1 }), t: Long("6") },
      optimeDate: ISODate("2023-10-26T09:38:54.000Z"),
      lastAppliedWallTime: ISODate("2023-10-26T09:38:54.458Z"),
      lastDurableWallTime: ISODate("2023-10-26T09:38:54.458Z"),
      syncSourceHost: '',
      syncSourceId: -1,
      infoMessage: '',
      electionTime: Timestamp({ t: 1698311944, i: 1 }),
      electionDate: ISODate("2023-10-26T09:19:04.000Z"),
      configVersion: 1,
      configTerm: 6,
      self: true,
      lastHeartbeatMessage: ''
    },
    {
      _id: 1,
      name: 'MongoSRD02.localdomain:27011',
      health: 1,
      state: 2,
      stateStr: 'SECONDARY',
      uptime: 1204,
      optime: { ts: Timestamp({ t: 1698313134, i: 1 }), t: Long("6") },
      optimeDurable: { ts: Timestamp({ t: 1698313134, i: 1 }), t: Long("6") },
      optimeDate: ISODate("2023-10-26T09:38:54.000Z"),
      optimeDurableDate: ISODate("2023-10-26T09:38:54.000Z"),
      lastAppliedWallTime: ISODate("2023-10-26T09:38:54.458Z"),
      lastDurableWallTime: ISODate("2023-10-26T09:38:54.458Z"),
      lastHeartbeat: ISODate("2023-10-26T09:38:57.456Z"),
      lastHeartbeatRecv: ISODate("2023-10-26T09:38:57.449Z"),
      pingMs: Long("0"),
      lastHeartbeatMessage: '',
      syncSourceHost: 'MongoSRD01.localdomain:27011',
      syncSourceId: 0,
      infoMessage: '',
      configVersion: 1,
      configTerm: 6
    },
    {
      _id: 2,
      name: 'MongoArbiter.localdomain:27011',
      health: 1,
      state: 7,
      stateStr: 'ARBITER',
      uptime: 1204,
      lastHeartbeat: ISODate("2023-10-26T09:38:57.410Z"),
      lastHeartbeatRecv: ISODate("2023-10-26T09:38:57.409Z"),
      pingMs: Long("0"),
      lastHeartbeatMessage: '',
      syncSourceHost: '',
      syncSourceId: -1,
      infoMessage: '',
      configVersion: 1,
      configTerm: 6
    }
  ],
  ok: 1,
  '$clusterTime': {
    clusterTime: Timestamp({ t: 1698313137, i: 2 }),
    signature: {
      hash: Binary.createFromBase64("AAAAAAAAAAAAAAAAAAAAAAAAAAA=", 0),
      keyId: Long("0")
    }
  },
  operationTime: Timestamp({ t: 1698313134, i: 1 })
}
[root@MongoSRD01 ~]# mongosh --port 27012
Current Mongosh Log ID: 653a3437d23f8613e506cd4e
RS2 [direct: primary] test> rs.status()
{
  set: 'RS2',
  date: ISODate("2023-10-26T09:41:13.929Z"),
  myState: 1,
  term: Long("5"),
  syncSourceHost: '',
  syncSourceId: -1,
  heartbeatIntervalMillis: Long("2000"),
  majorityVoteCount: 2,
  writeMajorityCount: 2,
  votingMembersCount: 3,
  writableVotingMembersCount: 2,
  optimes: {
    lastCommittedOpTime: { ts: Timestamp({ t: 1698313264, i: 1 }), t: Long("5") },
    lastCommittedWallTime: ISODate("2023-10-26T09:41:04.756Z"),
    readConcernMajorityOpTime: { ts: Timestamp({ t: 1698313264, i: 1 }), t: Long("5") },
    appliedOpTime: { ts: Timestamp({ t: 1698313264, i: 1 }), t: Long("5") },
    durableOpTime: { ts: Timestamp({ t: 1698313264, i: 1 }), t: Long("5") },
    lastAppliedWallTime: ISODate("2023-10-26T09:41:04.756Z"),
    lastDurableWallTime: ISODate("2023-10-26T09:41:04.756Z")
  },
  lastStableRecoveryTimestamp: Timestamp({ t: 1698313254, i: 1 }),
  electionCandidateMetrics: {
    lastElectionReason: 'priorityTakeover',
    lastElectionDate: ISODate("2023-10-26T09:19:14.628Z"),
    electionTerm: Long("5"),
    lastCommittedOpTimeAtElection: { ts: Timestamp({ t: 1698311951, i: 14006 }), t: Long("4") },
    lastSeenOpTimeAtElection: { ts: Timestamp({ t: 1698311951, i: 14006 }), t: Long("4") },
    numVotesNeeded: 2,
    priorityAtElection: 3,
    electionTimeoutMillis: Long("10000"),
    priorPrimaryMemberId: 1,
    numCatchUpOps: Long("0"),
    newTermStartDate: ISODate("2023-10-26T09:19:14.643Z"),
    wMajorityWriteAvailabilityDate: ISODate("2023-10-26T09:19:14.670Z")
  },
  members: [
    {
      _id: 0,
      name: 'MongoSRD01.localdomain:27012',
      health: 1,
      state: 1,
      stateStr: 'PRIMARY',
      uptime: 1331,
      optime: { ts: Timestamp({ t: 1698313264, i: 1 }), t: Long("5") },
      optimeDate: ISODate("2023-10-26T09:41:04.000Z"),
      lastAppliedWallTime: ISODate("2023-10-26T09:41:04.756Z"),
      lastDurableWallTime: ISODate("2023-10-26T09:41:04.756Z"),
      syncSourceHost: '',
      syncSourceId: -1,
      infoMessage: '',
      electionTime: Timestamp({ t: 1698311954, i: 1 }),
      electionDate: ISODate("2023-10-26T09:19:14.000Z"),
      configVersion: 1,
      configTerm: 5,
      self: true,
      lastHeartbeatMessage: ''
    },
    {
      _id: 1,
      name: 'MongoSRD02.localdomain:27012',
      health: 1,
      state: 2,
      stateStr: 'SECONDARY',
      uptime: 1330,
      optime: { ts: Timestamp({ t: 1698313264, i: 1 }), t: Long("5") },
      optimeDurable: { ts: Timestamp({ t: 1698313264, i: 1 }), t: Long("5") },
      optimeDate: ISODate("2023-10-26T09:41:04.000Z"),
      optimeDurableDate: ISODate("2023-10-26T09:41:04.000Z"),
      lastAppliedWallTime: ISODate("2023-10-26T09:41:04.756Z"),
      lastDurableWallTime: ISODate("2023-10-26T09:41:04.756Z"),
      lastHeartbeat: ISODate("2023-10-26T09:41:13.910Z"),
      lastHeartbeatRecv: ISODate("2023-10-26T09:41:13.590Z"),
      pingMs: Long("1"),
      lastHeartbeatMessage: '',
      syncSourceHost: 'MongoSRD01.localdomain:27012',
      syncSourceId: 0,
      infoMessage: '',
      configVersion: 1,
      configTerm: 5
    },
    {
      _id: 2,
      name: 'MongoArbiter.localdomain:27012',
      health: 1,
      state: 7,
      stateStr: 'ARBITER',
      uptime: 1330,
      lastHeartbeat: ISODate("2023-10-26T09:41:13.910Z"),
      lastHeartbeatRecv: ISODate("2023-10-26T09:41:13.904Z"),
      pingMs: Long("3"),
      lastHeartbeatMessage: '',
      syncSourceHost: '',
      syncSourceId: -1,
      infoMessage: '',
      configVersion: 1,
      configTerm: 5
    }
  ],
  ok: 1,
  '$clusterTime': {
    clusterTime: Timestamp({ t: 1698313272, i: 1 }),
    signature: {
      hash: Binary.createFromBase64("AAAAAAAAAAAAAAAAAAAAAAAAAAA=", 0),
      keyId: Long("0")
    }
  },
  operationTime: Timestamp({ t: 1698313264, i: 1 })
}
</code></pre>
Экземпляры репликасетов RS1 и RS2 на ноде MongoSRD01 стали автоматически Primary поскольку был указан приоритет при инициализации репликасета.
Проверим количество полей после восстановления работы ВМ MongoSRD01 на репликасетах RS1 и RS2:
<pre><code>RS1 [direct: primary] bank> db.tickets.count()
62809
RS2 [direct: primary] bank> db.tickets.count()
DeprecationWarning: Collection.count() is deprecated. Use countDocuments or estimatedDocumentCount.
87191</code></pre>
В сумме 150000 строк = 50000 изначальных и 100000 дополнительно созданных при отключенной ВМ - репликасет работает.

Так же было проверено: 
  * при двух выключенных экземплярах configsrv на ноде MongoCFG есть возможность подключения mongos к конфигу
  
  * при всех выключенных экземплярах configsrv запросы с mongos не доходят до шардов выдает ошибку о недоступности репликисета
  <pre><code>[direct: mongos] test> use bank
switched to db bank
[direct: mongos] bank> db.tickets.count()
DeprecationWarning: Collection.count() is deprecated. Use countDocuments or estimatedDocumentCount.
MongoServerError: failed on: RS2 :: caused by :: could not get updated shard list from config server :: caused by :: Could not find host matching read preference { mode: "nearest" } for set RScfg</code></pre>
  
  * Отключил оба экземпляра mongos на ноде MongoRouter и залил 100000 полей напрямую в RS1(был произведен откат всех нод к КТ с 50000 записей)
  <pre><code>RS1 [direct: primary] bank> for (var i=0; i<100000; i++) { db.tickets.insert({name: "Max ammout of cost tickets", amount: Math.random()*100}) }
DeprecationWarning: Collection.insert() is deprecated. Use insertOne, insertMany, or bulkWrite.
Uncaught:
MongoBulkWriteError: The critical section for bank.tickets is acquired with reason: { command: "moveChunk", fromShard: "RS1", toShard: "RS2" }
Result: BulkWriteResult {
  insertedCount: 0,
  matchedCount: 0,
  modifiedCount: 0,
  deletedCount: 0,
  upsertedCount: 0,
  upsertedIds: {},
  insertedIds: { '0': ObjectId("653b62fde90cc85944529926") }
}
Write Errors: [
  WriteError {
    err: {
      index: 0,
      code: 13388,
      errmsg: 'The critical section for bank.tickets is acquired with reason: { command: "moveChunk", fromShard: "RS1", toShard: "RS2" }',
      errInfo: undefined,
      op: {
        name: 'Max ammout of cost tickets',
        amount: 52.241712195266324,
        _id: ObjectId("653b62fde90cc85944529926")
      }
    }
  }
]
</code></pre>
Выдало ошибку о перемещении данных из шарда RS1 в RS2 по причине отключеного балансировщика mongos
Проверим статистику коллекции на RS1 и RS2:
<pre><code>RS1 [direct: primary] bank> db.tickets.stats()
--/--/--(не стал все строки копировать оставил только основные)
      nindexes: 2,
      indexBuilds: [],
      totalIndexSize: 7999488,
      indexSizes: { _id_: 2301952, amount_1: 5697536 },
      totalSize: 10432512,
      scaleFactor: 1
    }
  },
  sharded: false, 
  size: 6117150,
  count: 81562,
  numOrphanDocs: 13990, ---------------Появились осиротевшие документы
  storageSize: 2433024,
  totalIndexSize: 7999488,
  totalSize: 10432512,
  indexSizes: { _id_: 2301952, amount_1: 5697536 },
  avgObjSize: 75,
  ns: 'bank.tickets',
  nindexes: 2,
  scaleFactor: 1
  RS2 [direct: primary] bank> db.tickets.stats()
  --/--/--
        nindexes: 2,
      indexBuilds: [],
      totalIndexSize: 4341760,
      indexSizes: { _id_: 2523136, amount_1: 1818624 },
      totalSize: 7032832,
      scaleFactor: 1
    }
  },
  sharded: false,
  size: 3750750,
  count: 50010,
  numOrphanDocs: 0,
  storageSize: 2691072,
  totalIndexSize: 4341760,
  totalSize: 7032832,
  indexSizes: { _id_: 2523136, amount_1: 1818624 },
  avgObjSize: 75,
  ns: 'bank.tickets',
  nindexes: 2,
  scaleFactor: 1
</code></pre>
Перемещение по шардам произошло частично и появились осиротевшие документы. После запуска вновь бансировщика mongos произошло перераспределение данных и по шардам:
<pre><code>RS1 [direct: primary] bank> db.tickets.stats()
--/--/--
      nindexes: 2,
      indexBuilds: [],
      totalIndexSize: 9605120,
      indexSizes: { _id_: 3907584, amount_1: 5697536 },
      totalSize: 13877248,
      scaleFactor: 1
    }
  },
  sharded: false,
  size: 5067900,
  count: 67572,
  numOrphanDocs: 0,
  storageSize: 4272128,
  totalIndexSize: 9605120,
  totalSize: 13877248,
  indexSizes: { _id_: 3907584, amount_1: 5697536 },
  avgObjSize: 75,
  ns: 'bank.tickets',
  nindexes: 2,
  scaleFactor: 1
RS2 [direct: primary] bank> db.tickets.stats()
--/--/--
      nindexes: 2,
      indexBuilds: [],
      totalIndexSize: 4341760,
      indexSizes: { _id_: 2523136, amount_1: 1818624 },
      totalSize: 7032832,
      scaleFactor: 1
    }
  },
  sharded: false,
  size: 3750750,
  count: 50010,
  numOrphanDocs: 0,
  storageSize: 2691072,
  totalIndexSize: 4341760,
  totalSize: 7032832,
  indexSizes: { _id_: 2523136, amount_1: 1818624 },
  avgObjSize: 75,
  ns: 'bank.tickets',
  nindexes: 2,
  scaleFactor: 1
</code></pre>

4) Настроим аутентификацию и многоролевой доступ

Создадим разных пользователей на репликасете RS1, для этого зайдем на экземпляр с ноды MongoSRD01:
<pre><code>[root@MongoSRD01 ~]# mongosh --port 27011
Current Mongosh Log ID: 653b80582a41290b262bf814
RS1 [direct: primary] test> use admin   --------------переходим в БД админ
switched to db admin</code></pre>
Создадим роль superRoot с прилилегиями любых действий на любых объектах БД 
<pre><code>RS1 [direct: primary] admin> db.createRole( { role: "superRoot", privileges:[ { resource: {anyResource:true}, actions: ["anyAction"]} ], roles:[] })
{
  ok: 1,
  '$clusterTime': {
    clusterTime: Timestamp({ t: 1698398763, i: 4 }),
    signature: {
      hash: Binary.createFromBase64("AAAAAAAAAAAAAAAAAAAAAAAAAAA=", 0),
      keyId: Long("0")
    }
  },
  operationTime: Timestamp({ t: 1698398763, i: 4 })
}
</code></pre>
Проверим список всех ролей
<pre><code>RS1 [direct: primary] admin> show roles
[
  {
    role: '__system',
    db: 'admin',
    isBuiltin: true,
    roles: [],
    inheritedRoles: []
  },
  {
    role: 'enableSharding',
    db: 'admin',
    isBuiltin: true,
    roles: [],
    inheritedRoles: []
  },
  {
    role: 'backup',
    db: 'admin',
    isBuiltin: true,
    roles: [],
    inheritedRoles: []
  },
  {
    role: 'clusterAdmin',
    db: 'admin',
    isBuiltin: true,
    roles: [],
    inheritedRoles: []
  },
  {
    role: 'readAnyDatabase',
    db: 'admin',
    isBuiltin: true,
    roles: [],
    inheritedRoles: []
  },
  {
    role: 'read',
    db: 'admin',
    isBuiltin: true,
    roles: [],
    inheritedRoles: []
  },
  {
    role: 'readWriteAnyDatabase',
    db: 'admin',
    isBuiltin: true,
    roles: [],
    inheritedRoles: []
  },
  {
    role: '__queryableBackup',
    db: 'admin',
    isBuiltin: true,
    roles: [],
    inheritedRoles: []
  },
  {
    role: 'root',
    db: 'admin',
    isBuiltin: true,
    roles: [],
    inheritedRoles: []
  },
  {
    role: 'restore',
    db: 'admin',
    isBuiltin: true,
    roles: [],
    inheritedRoles: []
  },
  {
    role: 'dbAdmin',
    db: 'admin',
    isBuiltin: true,
    roles: [],
    inheritedRoles: []
  },
  {
    role: 'readWrite',
    db: 'admin',
    isBuiltin: true,
    roles: [],
    inheritedRoles: []
  },
  {
    role: 'userAdminAnyDatabase',
    db: 'admin',
    isBuiltin: true,
    roles: [],
    inheritedRoles: []
  },
  {
    role: 'hostManager',
    db: 'admin',
    isBuiltin: true,
    roles: [],
    inheritedRoles: []
  },
  {
    role: 'clusterManager',
    db: 'admin',
    isBuiltin: true,
    roles: [],
    inheritedRoles: []
  },
  {
    role: 'directShardOperations',
    db: 'admin',
    isBuiltin: true,
    roles: [],
    inheritedRoles: []
  },
  {
    role: 'userAdmin',
    db: 'admin',
    isBuiltin: true,
    roles: [],
    inheritedRoles: []
  },
  {
    role: 'dbOwner',
    db: 'admin',
    isBuiltin: true,
    roles: [],
    inheritedRoles: []
  },
  {
    role: 'clusterMonitor',
    db: 'admin',
    isBuiltin: true,
    roles: [],
    inheritedRoles: []
  },
  {
    role: 'dbAdminAnyDatabase',
    db: 'admin',
    isBuiltin: true,
    roles: [],
    inheritedRoles: []
  },
  {
    _id: 'admin.superRoot', -----------------------------созданная нами роль
    role: 'superRoot',
    db: 'admin',
    roles: [],
    isBuiltin: false,
    inheritedRoles: []
  }
]</code></pre>
Создадим пользователя и привяжем к нему роль superRoot:
<pre><code>RS1 [direct: primary] admin> db.createUser({ user: "AdminRS", pwd: "SA123456", roles: ["superRoot"] })
{
  ok: 1,
  '$clusterTime': {
    clusterTime: Timestamp({ t: 1698399343, i: 3 }),
    signature: {
      hash: Binary.createFromBase64("AAAAAAAAAAAAAAAAAAAAAAAAAAA=", 0),
      keyId: Long("0")
    }
  },
  operationTime: Timestamp({ t: 1698399343, i: 3 })
}</code></pre>
Зайдем на репликасет по УЗ AdminRS:
<pre><code>[root@MongoSRD01 ~]# mongosh --port 27011 -u AdminRS -p SA123456
Current Mongosh Log ID: 653b851a082c1ea77ceb22be
RS1 [direct: primary] test> use bank
switched to db bank
RS1 [direct: primary] bank> db.tickets.count()
DeprecationWarning: Collection.count() is deprecated. Use countDocuments or estimatedDocumentCount.
67572
</code></pre>
Перезапустим экземпляр репликасета с параметром аутентификации и попробуем войти под пользователем AdminRS:
<pre><code>
[root@MongoSRD01 ~]# mongod --shardsvr --dbpath /home/mongo/sh1 --port 27011 --auth --bind_ip 0.0.0.0 --replSet RS1 --fork --logpath /home/mongo/sh1/sh1.log --pidfilepath /home/mongo/sh1/sh1.pid
BadValue: security.keyFile is required when authorization is enabled with replica sets
try 'mongod --help' for more information
</code></pre>
Выдало ошибку поскольку для репликисетов требуется создание ключевого файла и распределение ключа файла на всех связанных экземплярах репликасетов:
<pre><code>[root@MongoSRD01 ~]# openssl rand -base64 756 > keyfile -------------создали ключ-файл
scp keyfile root@192.168.0.210:/home/mongo------------отправили на ВМ участвующие в репликасете
scp keyfile root@192.168.0.212:/home/mongo
scp keyfile root@192.168.0.213:/home/mongo
scp keyfile root@192.168.0.214:/home/mongo
</code></pre>
Дали права на файл владельца mongod:mongod и только чтение для владельца(делал через mc).
Перезапустили все экземпляры c --keyFile на всех ВМ начиная(важно) с конфига MongoCFG:
<pre><code>[root@MongoCFG ~]# ps aux | grep mongo| grep -Ev "grep"
mongod    1122  0.7  8.1 2701848 148876 ?      Ssl  окт26   9:35 /usr/bin/mongod -f /etc/mongod.conf
root     18155  2.2  7.9 3363168 144948 ?      SLl  16:36   1:03 mongod --configsvr --dbpath /home/mongo/dbc1 --port 27001 --keyFile /home/mongo/keyfile --bind_ip_all --replSet RScfg --fork --logpath /home/mongo/dbc1/dbc1.log --pidfilepath /home/mongo/dbc1/dbc1.pid
root     18301  2.0  8.2 3261140 149852 ?      SLl  16:37   0:56 mongod --configsvr --dbpath /home/mongo/dbc2 --port 27002 --keyFile /home/mongo/keyfile --bind_ip_all --replSet RScfg --fork --logpath /home/mongo/dbc2/dbc2.log --pidfilepath /home/mongo/dbc2/dbc2.pid
root     18504  2.0  7.3 3252420 132604 ?      SLl  16:37   0:56 mongod --configsvr --dbpath /home/mongo/dbc3 --port 27003 --keyFile /home/mongo/keyfile --bind_ip_all --replSet RScfg --fork --logpath /home/mongo/dbc3/dbc3.log --pidfilepath /home/mongo/dbc3/dbc3.pid

[root@MongoSRD01 ~]# ps aux | grep mongo| grep -Ev "grep"
root     29959  1.9  9.2 3295272 168304 ?      SLl  16:38   0:54 mongod --shardsvr --dbpath /home/mongo/sh1 --port 27011 --keyFile /home/mongo/sh1/keyfile --bind_ip_all --replSet RS1 --fork --logpath /home/mongo/sh1/sh1.log --pidfilepath /home/mongo/sh1/sh1.pid
root     30178  2.0  9.4 3286212 172512 ?      SLl  16:40   0:54 mongod --shardsvr --dbpath /home/mongo/sh2 --port 27012 --keyFile /home/mongo/sh1/keyfile --bind_ip_all --replSet RS2 --fork --logpath /home/mongo/sh2/sh2.log --pidfilepath /home/mongo/sh2/sh2.pid

[root@MongoSRD02 ~]# ps aux | grep mongo| grep -Ev "grep"
mongod    1125  0.9  8.1 2702456 147812 ?      Ssl  окт26  12:00 /usr/bin/mongod -f /etc/mongod.conf
root     31006  1.7  9.5 3215524 172816 ?      SLl  16:41   0:46 mongod --shardsvr --dbpath /home/mongo/sh1 --port 27011 --keyFile /home/mongo/sh1/keyfile --bind_ip_all --replSet RS1 --fork --logpath /home/mongo/sh1/sh1.log --pidfilepath /home/mongo/sh1/sh1.pid
root     31264  1.7  8.2 3201236 150000 ?      SLl  16:43   0:44 mongod --shardsvr --dbpath /home/mongo/sh2 --port 27012 --keyFile /home/mongo/sh2/keyfile --bind_ip_all --replSet RS2 --fork --logpath /home/mongo/sh2/sh2.log --pidfilepath /home/mongo/sh2/sh2.pid

[root@MongoArbiter ~]# ps aux | grep mongo| grep -Ev "grep"
mongod    1123  0.9  8.1 2696708 147988 ?      Ssl  окт26  11:14 /usr/bin/mongod -f /etc/mongod.conf
root     29682  1.1  7.1 2954808 129660 ?      SLl  16:47   0:27 mongod --shardsvr --dbpath /home/mongo/sh1 --port 27011 --keyFile /home/mongo/keyfile --bind_ip_all --replSet RS1 --fork --logpath /home/mongo/sh1/sh1.log --pidfilepath /home/mongo/sh1/sh1.pid
root     29793  1.1  7.2 2962072 132392 ?      SLl  16:47   0:25 mongod --shardsvr --dbpath /home/mongo/sh2 --port 27012 --keyFile /home/mongo/keyfile --bind_ip_all --replSet RS2 --fork --logpath /home/mongo/sh2/sh2.log --pidfilepath /home/mongo/sh2/sh2.pid

[root@MongoRouter ~]# ps aux | grep mongo| grep -Ev "grep"
root     14115  0.8  3.1 2632852 56904 ?       SLl  16:53   0:17 mongos --configdb RScfg/MongoCFG.localdomain:27001,MongoCFG.localdomain:27002,MongoCFG.localdomain:27003 --port 27000 --keyFile /home/mongo/keyfile --bind_ip_all --fork --logpath /home/mongo/dbc1/dbs1.log --pidfilepath /home/mongo/dbc1/dbs1.pid
root     14204  0.8  2.5 2632720 45772 ?       SLl  16:54   0:16 mongos --configdb RScfg/MongoCFG.localdomain:27001,MongoCFG.localdomain:27002,MongoCFG.localdomain:27003 --port 27100 --keyFile /home/mongo/keyfile --bind_ip_all --fork --logpath /home/mongo/dbc2/dbs2.log --pidfilepath /home/mongo/dbc2/dbs2.pid
</code></pre>
Соответственно при такой настройке требуется заново создавать пользователя, заходим через балансировщик:
<pre><code>[root@MongoRouter ~]# mongosh --port 27000
Current Mongosh Log ID: 653bcaa433a0ee7dcb626fe0
[direct: mongos] test> use admin
switched to db admin
[direct: mongos] admin> db.createUser({user: "admindz03",pwd: "admin123456",roles: [{role: "userAdminAnyDatabase",db: "admin"},{role: "clusterAdmin",db: "admin"},{role: "root",db: "admin"}]})
{
  ok: 1,
  '$clusterTime': {
    clusterTime: Timestamp({ t: 1698417329, i: 5 }),
    signature: {
      hash: Binary.createFromBase64("by9VBvCghcxga53S+hiAF1r9PJ4=", 0),
      keyId: Long("7293534696938405910")
    }
  },
  operationTime: Timestamp({ t: 1698417329, i: 5 })
}</code></pre>
Заходим под новым пользователем и проверяем права:
<pre><code>[root@MongoRouter ~]# mongosh --port 27000 -u admindz03 -p admin123456
Current Mongosh Log ID: 653bcad541d0bc2838a6a7d7
[direct: mongos] test> use admin
switched to db admin
[direct: mongos] admin> show users
[
  {
    _id: 'admin.admindz03',
    userId: new UUID("6a4257e6-6430-490b-8055-6e9bd2b91191"),
    user: 'admindz03',
    db: 'admin',
    roles: [
      { role: 'userAdminAnyDatabase', db: 'admin' },
      { role: 'clusterAdmin', db: 'admin' },
      { role: 'root', db: 'admin' }
    ],
    mechanisms: [ 'SCRAM-SHA-1', 'SCRAM-SHA-256' ]
  }
]
</code></pre>
Создадим пользователя с правами на чтение БД "bank":
<pre><code>[direct: mongos] admin> db.createUser({user: "w_user",pwd: "123456",roles: [{role: "read",db: "bank"}]})
{
  ok: 1,
  '$clusterTime': {
    clusterTime: Timestamp({ t: 1698417974, i: 1 }),
    signature: {
      hash: Binary.createFromBase64("5uRZhygMBZHuydkFLgx54uWZuCE=", 0),
      keyId: Long("7293534696938405910")
    }
  },
  operationTime: Timestamp({ t: 1698417974, i: 1 })
}
[direct: mongos] admin> show users
[
  {
    _id: 'admin.admindz03',
    userId: new UUID("6a4257e6-6430-490b-8055-6e9bd2b91191"),
    user: 'admindz03',
    db: 'admin',
    roles: [
      { role: 'userAdminAnyDatabase', db: 'admin' },
      { role: 'clusterAdmin', db: 'admin' },
      { role: 'root', db: 'admin' }
    ],
    mechanisms: [ 'SCRAM-SHA-1', 'SCRAM-SHA-256' ]
  },
  {
    _id: 'admin.w_user',
    userId: new UUID("e731c623-992c-4c04-ae8d-5fecfc026042"),
    user: 'w_user',
    db: 'admin',
    roles: [ { role: 'read', db: 'bank' } ],
    mechanisms: [ 'SCRAM-SHA-1', 'SCRAM-SHA-256' ]
  }
]
</code></pre>
Зайдем под пользователем w_user разными способами:
<pre><code>
[root@MongoRouter ~]# mongosh --port 27000 -u w_user -p 123456 --authenticationDatabase "bank"
Current Mongosh Log ID: 653bce4726cea5aa4957500e
Connecting to:          mongodb://<credentials>@127.0.0.1:27000/?directConnection=true&serverSelectionTimeoutMS=2000&authSource=bank&appName=mongosh+2.0.1
MongoServerError: Authentication failed.
[root@MongoRouter ~]# mongosh --port 27000 -u w_user -p 123456 --authenticationDatabase "admin"
Current Mongosh Log ID: 653bcdb5b05b2e4c4582e71a
[direct: mongos] test> use admin
switched to db admin
[direct: mongos] admin> show users
MongoServerError: not authorized on admin to execute command { usersInfo: 1, lsid: { id: UUID("20059386-3ed2-4e75-898b-bcfbff9b0562") }, $clusterTime: { clusterTime: Timestamp(1698418036, 1), signature: { hash: BinData(0, 5EF183E208E8A5ECBEC3208F26F7A85970747CC6), keyId: 7293534696938405910 } }, $db: "admin" }
[direct: mongos] admin> use bank
switched to db bank
[direct: mongos] bank> db.tickets.count()
DeprecationWarning: Collection.count() is deprecated. Use countDocuments or estimatedDocumentCount.
117582
</code></pre>
Со строкой подключения --authenticationDatabase "bank" не получилось подключится, так как пользователь был создан в БД "admin" и при использование параметра --authenticationDatabase "bank", идет обращение к БД "bank", в которой нет данных о пользователе w_user.